{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Talk to Your PDF: An Introduction to Retrieval Augmented Generation (RAG) for Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a hands-on introduction to the concept of Retrieval Augmented Generation (RAG). We'll build a simple application that allows you to \"talk\" to a PDF document by asking it questions in natural language. This is a powerful technique that combines the strengths of large language models (LLMs) with the ability to retrieve information from your own documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is RAG?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG is a technique that enhances the capabilities of LLMs by allowing them to access and incorporate information from external knowledge sources. Instead of relying solely on the knowledge the LLM was trained on, we first retrieve relevant information from a specific dataset (in our case, a PDF document) and then use that information to generate a more accurate and context-aware response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is RAG useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  **Reduces Hallucinations:** LLMs can sometimes generate plausible-sounding but incorrect or nonsensical information (known as \"hallucinations\"). RAG grounds the LLM's responses in a specific context, reducing the likelihood of such errors.\n",
    "2.  **Uses Up-to-Date Information:** RAG allows LLMs to access information that wasn't part of their original training data, making it possible to work with real-time or proprietary data.\n",
    "3.  **Increases Trust and Verifiability:** Because the generated responses are based on retrieved documents, users can verify the information and trace it back to the source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's install the necessary libraries. We'll be using:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   `google-generativeai`: To interact with the Gemini API.\n",
    "*   `pypdf`: To read and extract text from PDF files.\n",
    "*   `langchain`: A framework for developing applications powered by language models.\n",
    "*   `faiss-cpu`: For efficient similarity search in our vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U google-generativeai pypdf langchain faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure your Gemini API Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the Gemini API, you'll need an API key. If you don't have one, you can get it from the Google AI Studio: [https://aistudio.google.com/app/apikey](https://aistudio.google.com/app/apikey). Once you have your key, you can add it to the Colab secrets manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from google.colab import userdata\n",
    "from getpass import getpass\n",
    "\n",
    "try:\n",
    "    # Use 'GEMINI_API_KEY' as the secret name\n",
    "    api_key = userdata.get('GEMINI_API_KEY')\n",
    "    genai.configure(api_key=api_key)\n",
    "except userdata.SecretNotFoundError as e:\n",
    "    print('GEMINI_API_KEY not found in Colab secrets. Please create it.')\n",
    "    api_key = getpass('Or, enter your Gemini API key: ')\n",
    "    genai.configure(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload your PDF"
   ]
  },
    {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print('Please upload your PDF file')\n",
    "uploaded = files.upload()\n",
    "\n",
    "if len(uploaded.keys()) > 0:\n",
    "    pdf_path = list(uploaded.keys())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Processing the PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load the PDF file you uploaded and extract its text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document.loaders import PyPDFLoader\n",
    "\n",
    "if 'pdf_path' in locals():\n",
    "    try:\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        pages = loader.load_and_split()\n",
    "        print(f\"Successfully loaded {len(pages)} pages from the PDF.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading PDF: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create a vector store from the extracted text. A vector store is a database that stores text as numerical representations (embeddings). This allows us to perform efficient similarity searches to find the most relevant text chunks for a given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import GoogleGenerativeAiEmbeddings\n",
    "\n",
    "if 'pages' in locals():\n",
    "    embeddings = GoogleGenerativeAiEmbeddings(model=\"models/embedding-001\")\n",
    "    vector_store = FAISS.from_documents(pages, embeddings)\n",
    "    print(\"Vector store created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the RAG Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build the RAG chain. The chain will:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Take a user's question.\n",
    "2.  Retrieve relevant documents from the vector store.\n",
    "3.  Pass the question and the retrieved documents to the Gemini model.\n",
    "4.  Generate a response based on the provided context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import GoogleGenerativeAI\n",
    "\n",
    "if 'vector_store' in locals():\n",
    "    llm = GoogleGenerativeAI(model=\"gemini-pro\", temperature=0.3)\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vector_store.as_retriever(),\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    print(\"RAG chain created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asking Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to ask some questions to your PDF! Try asking questions that can be answered from the content of your document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'qa_chain' in locals():\n",
    "    question = \"What is the main topic of the document?\"\n",
    "    result = qa_chain({\"query\": question})\n",
    "    print(\"Question:\", result[\"query\"])\n",
    "    print(\"Answer:\", result[\"result\"])\n",
    "    print(\"Source Documents:\", [doc.metadata.get(\"page\", \"N/A\") for doc in result[\"source_documents\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'qa_chain' in locals():\n",
    "    # Try another question\n",
    "    question = \"Summarize the key findings.\"\n",
    "    result = qa_chain({\"query\": question})\n",
    "    print(\"Question:\", result[\"query\"])\n",
    "    print(\"Answer:\", result[\"result\"])\n",
    "    print(\"Source Documents:\", [doc.metadata.get(\"page\", \"N/A\") for doc in result[\"source_documents\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You've successfully built a RAG application that allows you to chat with your PDF. This is a powerful technique that can be applied to a wide range of applications, from customer support chatbots to research assistants. Feel free to experiment with different PDFs and questions to explore the capabilities of RAG."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}